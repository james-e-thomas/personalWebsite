---
title: Automating the downloading of files from the CMA website using R
author: ''
date: '2021-05-04'
slug: []
categories: []
tags:
  - rvest
  - scraping
  - r
  - cma
description: ''
keywords: []
math: no
toc: yes
---

This post is slightly specific to my work. The Competition and Markets Authority ("CMA") will often publish documents at various points during its cases. I was prompted to write this short piece of code to automate the downloading of these documents when the CMA published 80+ responses from stakeholders to a particular set of working papers.

The code uses the [`rvest`](https://rvest.tidyverse.org/) package to scrape the CMA's website and the `utils` package to download the files.

```
rm(list = ls())

library(rvest)
library(magrittr)
library(utils)

setwd("C:\\Users\\james\\Documents\\Online Platforms Final Report and Appendices")                                         

# -----------------------------------------------------------------------------

# Step 1: Get HTML code
webpage_html <- read_html("https://www.gov.uk/cma-cases/online-platforms-and-digital-advertising-market-study")              

# Step 2: Find relevant part of HTML code
section_html <- webpage_html %>%
  html_node("ul:nth-child(10)") %>%
  html_nodes("a")

# Step 3: Find links in HTML
links <- section_html %>%
  html_attr("href")

## Step 4: Extract document names from webpage text
doc_names <- section_html %>%
  html_text("href")
doc_names <- paste0(doc_names, ".pdf")
doc_names <- gsub(":", "", doc_names)

# Step 5: Download files from links to working directory  
for (i in seq_along(links)) {
  download.file(links[i], doc_names[i], mode = "wb")
}
```

I explain the code below.

## Step 1: Get HTML code

Websites are written in a language called HTML. To view the HTML of any website, you can simply right click and select 'Inspect' (or press `Ctrl`+`Shift`+`I`). 

All [`read_html()`](https://xml2.r-lib.org/reference/read_xml.html) is doing is going to the address inputted and retrieving the HTML of that webpage.

The file it downloads is an XML file. You can play around with it. You will see that it is a list of length 2 (because all HTML pages are made up of two sections: a head and a body).

```{r include=FALSE}
rm(list = ls())

library(rvest)
library(magrittr)
library(utils)

```

```{r}
webpage_html <- read_html("https://www.gov.uk/cma-cases/online-platforms-and-digital-advertising-market-study")  

webpage_html
```

The content of webpages are found in the second element of the list (i.e. in the body section). 

```{r}
xml_child(webpage_html, 2)
```

## Step 2: Find relevant part of HTML code

The next step is finding the documents we want to download within those 15 lines of code. But first let's take a step back.

</br>
<style>
div.blue { border-left: 6px solid #002e63; background-color:#eef3fa; padding-left: 30px; padding-right: 20px; padding-top: 0.5px; padding-bottom: 30px}
</style>
<div class = "blue">
### HTML elements/attributes and CSS selectors

HTML pages are made up of **elements**. These elements will typically include an opening tag and a closing tag. If you inspect the HTML of this text (do it!), you will see that it is a paragraph, denoted by an opening tag `<p>` and a closing tag `</p>`.

```{r eval=FALSE}
<p>
  HTML pages are made up of <strong>elements</strong>. These elements will typically include an opening tag and a closing tag. If you inspect the HTML of this text (do it!), you will see that it is a paragraph, denoted by an opening tag <code>&lt;p&gt;</code> and a closing tag <code>&lt;/p&gt;</code>.
</p>
```

There are many other types of elements. For example, headers (`<h1>`, `<h2>`, `<h3>` etc..), links (`<a>`), lists (`<ul>`, `<ol>`, `<li>`), images (`<img>`), dividers (`div`) and many more...

Tags can have **attributes**. The most common attributes are `id` and `class`. These two attributes can be used for example to style the element (e.g. a particular format/colour/style can be defined for elements with a particular `id` or `class`). This is done using Cascading Style Sheets ("CSS"), a language used to define the presentation of HTML documents (HTML- content structure and semantics; CSS- content style and layout).

In CSS, **selectors** can be used to select the HTML elements that you want to style. Similarly, they can be used to identify HTML elements that you want to scrape... The four most important selectors are (the following is taken directly from the `rvest` package's website):

- `p`: selects all `<p>` elements.
- `.title`: selects all elements with `class` "title".
- `p.special`: selects all `<p>` elements with `class` "special".
- `#title`: selects the element with the `id` attribute that equals "title".

This is by no means a complete introduction to HTML, CSS and webpages, but there are plenty of resources out there. If you are interested in learning how to scrape beyond this simple CMA example, this little tutorial isn't going to cut it! You could start with the `rvest` package's own introduction to [HTML and scraping basics](https://rvest.tidyverse.org/articles/rvest.html), but even this is likely to be limited. The [MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/HTML) produced by Mozilla (the company behind the Firefox web browser) are a good resource for getting familiar with HTML and CSS. In terms of scraping specifically though, I think the best way of learning is to work through some sort of textbook. I personally used Ryan Mitchell's Web Scraping with Python (though if you intend to use R, maybe a different textbook would be best!).
</div>

Anyway- back to the scraping. Let's take it line by line. [`html_node()`](https://rvest.tidyverse.org/reference/html_nodes.html) in this case looks for the first unordered list, `<ul>`, that is also the tenth **child** of its parent (an element's children are the elements nested within it- e.g. `<head>` and `<body>` are children of `<html>`). 

```{r}
section_html <- webpage_html %>%
  html_node("ul:nth-child(10)")

section_html
```

Let's also view the full HTML code by converting the list to character strings and concatenating them together. If you scroll to the right below, you will see the links to the pdfs and the text that appears on the CMA's website: 'Final report (1.7.20)', 'Appendix A: the legal framework (1.7.20)', Appendix B: summary of responses to our interim report consultation (1.7.20)' etc. 

```{r}
cat(as.character(section_html))
```

It might be worth taking another step back though. How do you go about finding the CSS selector for a particular section you might want to scrape? `ul:nth-child(10)` is somewhat specific and doesn't appear directly in the HTML code anywhere. Although this is true, it is possible to get the information from inspecting the webpage (`Ctrl`+`Shift`+`I`), clicking on the specific part of the webpage you are interested in and working out yourself how to uniquely identify it. In the case of downloading files from the CMA website, `ul:nth-child(x)` will usually do the job (provided they don't change the structure of their webpages). For scraping more generally, there will be other ways of identifying the part(s) of the webpage that you want (i.e. not necessarily by referencing the element and whether it is the nth child- see for example the four bullet points in the 'HTML elements/attributes and CSS selectors' section).

Although you can work it out yourself, a much simpler way to find the right CSS selector for basic scraping tasks is to use a helpful tool called CSS SelectorGadget.

</br>
<style>
div.blue { border-left: 6px solid #002e63; background-color:#eef3fa; padding-left: 30px; padding-right: 20px; padding-top: 0.5px; padding-bottom: 30px}
</style>
<div class = "blue">
### CSS SelectorGadget

SelectorGadget is a tool that helps you find the relevant CSS selector for the part of the webpage you want to scrape. You can either save the bookmark at [selectorgadget.com](https://selectorgadget.com/) or download the Chrome extension. Once you launch it, all you need to do is click on the webpage element you want to scrape. It will turn green and the tool will generate a minimal CSS selector for the element (and will highlight in yellow everything else that is matched by the selector). If the CSS selector also matches elements you are not interested in, simply click on elements you don't want to scrape. They will turn red and the tool will generate a more specific CSS selector (this is how we end up with something like `ul:nth-child(10)`).

</div>

Back to the code- ultimately, we want a list of links to the various pdfs. Links are denoted by the `<a>` tag. We thus search for this tag within the HTML code above.

```{r}
section_html <- section_html %>%
  html_nodes("a")

section_html
```

## Step 3: Find links in HTML

If you look at the code above, you will notice that the value of each link's `href` attribute is an address to a pdf. These can all be extracted using [`html_attr()`](https://rvest.tidyverse.org/reference/html_attr.html).

```{r}
links <- section_html %>%
  html_attr("href")

links
```

## Step 4: Extract document names from webpage text

In order to name the documents, we can use the text from the CMA's website. This can be extracted using [`html_text()`](https://rvest.tidyverse.org/reference/html_text.html). We then append the ".pdf" file extension and remove the colon (which isn't a valid character for a file name).

```{r}
doc_names <- section_html %>%
  html_text("href")
doc_names <- paste0(doc_names, ".pdf")
doc_names <- gsub(":", "", doc_names)

doc_names
```

## Step 5: Download files from links to working directory 

The final step is simply to download all the files using `download.file()` from the `utils` package. The first argument is the url to download from (Step 3) and the second is the name you want to give the file (Step 4). Remember to set your working directory up front, as that is where the documents will get saved.

```
for (i in seq_along(links)) {
  download.file(links[i], doc_names[i], mode = "wb")
}
```

## Final remarks

That should be enough information to run and understand the code. One thing to bear in mind though is that webpages get edited. As such, the CSS selector that identifies what you want one day may not do so the next. For example, if the CMA adds another set of pdfs to the case page, `ul:nth-child(10)` will no longer be the right CSS selector. Rather, it might now be `ul:nth-child(11)`.

And one final point. The code above is not actually my original code. It has been amended slightly to download the Final Report and all the various appendices of the CMA's [Market Study into Online Platforms and Digital Advertising](https://www.gov.uk/cma-cases/online-platforms-and-digital-advertising-market-study). To amend the code to download a different set of documents, only two lines have to be tweaked:

- The link to the CMA's case page (the first line in step 1)
- The particular section of the case page (the second line of step 2)